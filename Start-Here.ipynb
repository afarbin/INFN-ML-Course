{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Start-Here.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOVKNujNnNL-",
        "colab_type": "text"
      },
      "source": [
        "# INFN ML Course: Hands On\n",
        "\n",
        "## Structure of this Hands On\n",
        "\n",
        "This tutorial is targetting HEP researchers with some familarity with Python and aiming to learn enough Data Science, Machine Learning, and Deep Learning to apply the emerging tools and techniques to their research. The tutorial is organized into two threads. The primary thread, contained in this notebook, leads you through a typical HEP problem (a SUSY search with reconstructed objects), utilizing various python-based packages. As each package is introduced, you have an option of jumping to a concurrent thread consisting of notebooks that aim to give a brief primer for each package.\n",
        "\n",
        "Note that like most HEP tutorials, there are many sections where you can simply follow the instructions and execute example code like a robot (usualy via copy/paste into a terminal, but in this case using Jupyter notebooks). But this tutorial also aims to teach you key concepts in scientific computing, Machine Learning, and Deep Learning in python through exercises that require you to slow down, think critically, and apply what you read. We encourage you to try to work through the exercises, but you'll notice that you will find solutions in hidden cells for some of the exercises.\n",
        "\n",
        "The tutorial is divided into three sections: (TO BE UPDATED)\n",
        "\n",
        "A. <a href='#Basics'>Basics</a>\n",
        "   1. <a href='#Jupyter'>Notebooks</a>\n",
        "   2. <a href='#Numpy'>Numpy</a>\n",
        "   3. <a href='#HDF5'>HDF5</a>\n",
        "   \n",
        "B. <a href='#MachineLearning'>MachineLearning</a>\n",
        "   1. <a href='#Dataset'>Dataset</a>\n",
        "   2. <a href='#Pandas'>Pandas</a>\n",
        "   3. <a href='#Scikit-learn'>Scikit-learn</a>\n",
        "\n",
        "C. <a href='#DeepLearning'>DeepLearning</a>\n",
        "   1. <a href='#Keras'>Keras</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0h0yUK1nNMQ",
        "colab_type": "text"
      },
      "source": [
        "<a id='Basics'></a>\n",
        "# A. Basics\n",
        "\n",
        "Data Science in python usually starts with loading data into numpy tensors for manipulation in an interactive python session. While you can run the session in a terminal, interactive notebook systems (e.g. Jupyter) provide a nice web-based alternative environment for data analysis. These environments allow you to combine text, code, and results all in one interactive document in your browser. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZckLZxK3nNMT",
        "colab_type": "text"
      },
      "source": [
        "<a id='Notebooks'></a>\n",
        "## 1. Python Notebooks\n",
        "\n",
        "You are currently using Google Colab, which is a service hosted by Google that gives free access to computing resources (including GPUs) via a custom notebook environment. You can easily run a very similar system on your own laptop or computing resources using [Jupyter](https://jupyter.org).  An important note about Colab: if the sessions are currently in was spawned off of github, your notebook will be not saved and you will loose everything if you close your browser. The easiest way of persisting your notebooks is to copy them to your Google Drive by pressing the \"COPY TO DRIVE\" button above. \n",
        "\n",
        "Notebooks consist of cells that can hold text or code (usually python). This text that you are reading, was written into a text cell as simple text \"coding\" language known as Markdown. When this cell is run (either automatically at start of the notebook or manually by pressing shift-enter), the Markdown text is interpreted into nice looking text. Running a code cell will execute the code in that cell and give you the results. If you make a mistake, you can usually simply change the cell and re-run. But be aware that since you ran the mistaken cell already, whatever code was properly executed before your mistake/error, was already executed and has therefore changed your current python environment accordingly. In some cases this situation will be problematic, and you will need to rerun the notebook from the start by pressing restarting the Runtime (in Colab) or pressing the \"reload\" botton (next to the \"stop\" button) (in Jupyter).\n",
        "\n",
        "You are encouraged to add cells to this notebook (using the \"+\" button(s) on the tool bar) and play around a bit. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBESF5qnDWmW",
        "colab_type": "text"
      },
      "source": [
        "## Notebook Basics\n",
        "\n",
        "### Text Cells and Markdown\n",
        "You can write whatever you like into text cells. If you double-click on this cell, you'll be able to edit the Markdown that generates what is displayed once you run a text cell (e.g. using shift-enter). Once editting a cell, Colab gives you some buttons on top to help with formatting of the text. Jupyter usually doesn't. But Markdown is an easy language to master. See [this table](https://en.wikipedia.org/wiki/Markdown#Example) for a table that shows how to format text in Markdown.\n",
        "\n",
        "### Code Cells\n",
        "\n",
        "Whatever code you type into a code cell is executed by the notebook in a common python runtime instance that persists during your session, when you run that cell. For example, select the next cell and press shift-enter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYpqq56EGxwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "962a349c-0835-480a-d7f2-4df7d0b4d6a2"
      },
      "source": [
        "print \"Hello World\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8Dwp89Gatc",
        "colab_type": "text"
      },
      "source": [
        "You can execute shell commands in a code cell by starting a line with an exclamation mark \"!\" (aka \"bang\"). For example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnsZ5PkeGZxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "1ea4399d-bb0a-48c2-e9ff-c82de0f2c7cc"
      },
      "source": [
        "!touch A_File\n",
        "!ls "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " A_File  'rows.csv?accessType=DOWNLOAD'   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WFHlxf_HzKT",
        "colab_type": "text"
      },
      "source": [
        "### Exercise A.1.1\n",
        "\n",
        "Later in this tutorial, we will attempt to follow the first paper on Deep Learning in High Energy physics by P. Baldi, et al. This paper demonstrates that Deep Neural Networks can learn from raw data the features that are typically used by physicists for searches for exotics particles. The authors provide the data they used for this paper. They considered two benchmark scenarios. We will focus on the SUSY benchmark. \n",
        "\n",
        "The dataset is somewhat large and takes time to decompress, so let's download it and decompress it.\n",
        "\n",
        "Execute the following cell to use `wget` to download and decompress the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). If you are running in Google Drive, the data will be downloaded there. Note that you should only run this cell once... so we recommend you comment out the contents of the cell (using hash marks \"#\" on each line) to avoid redownloading the data every time you run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCYgkxBIDKWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "d96bc615-7185-4360-b5f9-561dee7b5543"
      },
      "source": [
        "\n",
        "# On MacOS:\n",
        "#!curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz > SUSY.csv.gz\n",
        "\n",
        "# Elsewhere:\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n",
        "  \n",
        "!gunzip SUSY.csv.gz\n",
        "\n",
        "!ls\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-15 15:39:29--  http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 922377711 (880M) [application/x-httpd-php]\n",
            "Saving to: ‘SUSY.csv.gz’\n",
            "\n",
            "SUSY.csv.gz         100%[===================>] 879.65M  43.3MB/s    in 21s     \n",
            "\n",
            "2019-05-15 15:39:50 (42.1 MB/s) - ‘SUSY.csv.gz’ saved [922377711/922377711]\n",
            "\n",
            " A_File  'rows.csv?accessType=DOWNLOAD'   sample_data   SUSY.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVk6MfosKFre",
        "colab_type": "text"
      },
      "source": [
        "You can look at the contents of the file using the unix `head` command and count the number of lines using the unix `wc` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUoEy55dKNLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "8c5fe816-7200-4320-d5ed-e79901e2693e"
      },
      "source": [
        "!head -5 SUSY.csv\n",
        "!wc -l SUSY.csv\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000000000000000000e+00,9.728614687919616699e-01,6.538545489311218262e-01,1.176224589347839355e+00,1.157156467437744141e+00,-1.739873170852661133e+00,-8.743090629577636719e-01,5.677649974822998047e-01,-1.750000417232513428e-01,8.100607395172119141e-01,-2.525521218776702881e-01,1.921887040138244629e+00,8.896374106407165527e-01,4.107718467712402344e-01,1.145620822906494141e+00,1.932632088661193848e+00,9.944640994071960449e-01,1.367815494537353516e+00,4.071449860930442810e-02\n",
            "1.000000000000000000e+00,1.667973041534423828e+00,6.419061869382858276e-02,-1.225171446800231934e+00,5.061022043228149414e-01,-3.389389812946319580e-01,1.672542810440063477e+00,3.475464344024658203e+00,-1.219136357307434082e+00,1.295456290245056152e-02,3.775173664093017578e+00,1.045977115631103516e+00,5.680512785911560059e-01,4.819284379482269287e-01,0.000000000000000000e+00,4.484102725982666016e-01,2.053557634353637695e-01,1.321893453598022461e+00,3.775840103626251221e-01\n",
            "1.000000000000000000e+00,4.448399245738983154e-01,-1.342980116605758667e-01,-7.099716067314147949e-01,4.517189264297485352e-01,-1.613871216773986816e+00,-7.686609029769897461e-01,1.219918131828308105e+00,5.040258169174194336e-01,1.831247568130493164e+00,-4.313853085041046143e-01,5.262832045555114746e-01,9.415140151977539062e-01,1.587535023689270020e+00,2.024308204650878906e+00,6.034975647926330566e-01,1.562373995780944824e+00,1.135454416275024414e+00,1.809100061655044556e-01\n",
            "1.000000000000000000e+00,3.812560737133026123e-01,-9.761453866958618164e-01,6.931523084640502930e-01,4.489588439464569092e-01,8.917528986930847168e-01,-6.773284673690795898e-01,2.033060073852539062e+00,1.533040523529052734e+00,3.046259880065917969e+00,-1.005284786224365234e+00,5.693860650062561035e-01,1.015211343765258789e+00,1.582216739654541016e+00,1.551914215087890625e+00,7.612152099609375000e-01,1.715463757514953613e+00,1.492256760597229004e+00,9.071890264749526978e-02\n",
            "1.000000000000000000e+00,1.309996485710144043e+00,-6.900894641876220703e-01,-6.762592792510986328e-01,1.589282631874084473e+00,-6.933256387710571289e-01,6.229069828987121582e-01,1.087561845779418945e+00,-3.817416727542877197e-01,5.892043709754943848e-01,1.365478992462158203e+00,1.179295063018798828e+00,9.682182073593139648e-01,7.285631299018859863e-01,0.000000000000000000e+00,1.083157896995544434e+00,4.342924803495407104e-02,1.154853701591491699e+00,9.485860168933868408e-02\n",
            "5000000 SUSY.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-j5_WP7Jfg8",
        "colab_type": "text"
      },
      "source": [
        "### Exercise A.1.2\n",
        "\n",
        "The SUSY dataset has 5 million examples (events), which is unnecessarily big for most of the ML algorithms we will be training. Use the unix `head` command and shell redirection to create a smaller file called \"SUSY-small.csv\" of 500,000 events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTghayNfKaTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkopTo3Kby_",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "!head -500000 SUSY.csv > SUSY-small.csv\n",
        "!wc -l SUSY-small.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s_3ehv5nNM4",
        "colab_type": "text"
      },
      "source": [
        "<a id='Numpy'></a>\n",
        "## 2. Numpy\n",
        "\n",
        "[Numpy](http://www.numpy.org) is the tensor manipulation package most commonly used in python-based scientific computing. Numpy tensor interface is also adopted by all packages that provide tensors (e.g. h5py, theano, TensorFlow, ...). You can use numpy as a Linear Algebra package like Matlab. Take a quick read of [Linear Algebra with Numpy](https://github.com/afarbin/INFN-ML-Course/blob/master/Numpy-Linear-Algebra.ipynb) to familiarize yourself with numpy. \n",
        "\n",
        "The size of a `numpy` tensor is encapsulated in its \"shape\" which is a tuple with length corresponding to the tensor's rank and each element specifying the size of each dimension. For example a tensor with shape (3,3) is a 3-by-3 matrix. \n",
        "\n",
        "Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvf3v-w39UFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "99e75b58-8575-423d-81e8-2821db5cd626"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "A= np.array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])\n",
        "\n",
        "print \"A = \",A\n",
        "\n",
        "print \"Shape of A \", A.shape\n",
        "\n",
        "print \"Length of shape of A \", len(A.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A =  [[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]]\n",
            "Shape of A  (3, 4)\n",
            "Length of shape of A  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_klUeRr_KUt",
        "colab_type": "text"
      },
      "source": [
        "And some examples of tensor manipulations that will help you with the exercises below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJvM7U-U_WLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0edb7612-deb1-49a4-a7af-6fc8420172eb"
      },
      "source": [
        "print \"Transpose of A:\"\n",
        "print A.transpose()\n",
        "\n",
        "B = A.reshape(2,6)\n",
        "\n",
        "print \"A reshaped into tensor B with shape (2,6):\"\n",
        "print B\n",
        "\n",
        "print \"Sum of A:\", np.sum(A)\n",
        "\n",
        "print \"Sum of A along axis 0:\"\n",
        "print np.sum(A,axis=0)\n",
        "\n",
        "print \"Sum of A along axis 1:\"\n",
        "print np.sum(A,axis=1)\n",
        "\n",
        "print \"Mean of B along axis 1:\"\n",
        "print np.mean(B,axis=1)\n",
        "\n",
        "print \"Multply A by a constant (2):\"\n",
        "print 2*A\n",
        "\n",
        "print \"Add constant 1 to every element of A:\"\n",
        "print A+1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transpose of A:\n",
            "[[ 1  5  9]\n",
            " [ 2  6 10]\n",
            " [ 3  7 11]\n",
            " [ 4  8 12]]\n",
            "A reshaped into tensor B with shape (2,6):\n",
            "[[ 1  2  3  4  5  6]\n",
            " [ 7  8  9 10 11 12]]\n",
            "Sum of A: 78\n",
            "Sum of A along axis 0:\n",
            "[15 18 21 24]\n",
            "Sum of A along axis 1:\n",
            "[10 26 42]\n",
            "Mean of B along axis 1:\n",
            "[3.5 9.5]\n",
            "Multply A by a constant (2):\n",
            "[[ 2  4  6  8]\n",
            " [10 12 14 16]\n",
            " [18 20 22 24]]\n",
            "Add constant 1 to every element of A:\n",
            "[[ 2  3  4  5]\n",
            " [ 6  7  8  9]\n",
            " [10 11 12 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e1Hl2B9ONa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "For more examples of creating and manipulating `numpy` tensors, we encourage you to read through this excellent [numpy walkthrough](https://github.com/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb) at some other time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUew_smnNM5",
        "colab_type": "text"
      },
      "source": [
        "### Exercise A.2.1\n",
        "\n",
        "Lets start with some basic reshape manipulations. Consider a classification task. We can imagine the training data X consisting of N examples each with M inputs, so the shape of X is (M,N). The output of the Neural Network for the training sample encodes the true class of each of the N examples in X, in a \"one-hot\" matrix of shape (N,C), where C is the number of classes and each row corresponds to the true class for the corresponding example in X. So for a given row Y[i], all elements are 0 except for the column corresponding to the true class.\n",
        "\n",
        "For example consider a classification task of separating between 4 classes. We'll call them A, B, C, and D.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIUozFMmnNM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Y=np.array( [ [0, 1, 0, 0], # Class B\n",
        "              [1, 0, 0, 0], # Class A\n",
        "              [0, 0, 0, 1], # Class C\n",
        "              [0, 0, 1, 0]  # Class D\n",
        "            ])\n",
        "\n",
        "print \"Shape of Y:\", Y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tZxUcVnNM9",
        "colab_type": "text"
      },
      "source": [
        "Lets imagine that we want to change to 2 classes instead by combining classes A with B and C with D. Use `np.reshape`, `np.sum`, and possibly `np.transpose` to create a new vector Y1. Hint: change the shape of Y into (8,2), sum along the correct axes, and change shape to (4,2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBkIqydXAnBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reGOgGtVnNM-",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "print \"Transpose:\", np.transpose(Y)\n",
        "print \"Reshape 8,2:\", np.transpose(Y).reshape((8,2))\n",
        "print \"Sum:\", np.sum(np.transpose(Y).reshape((8,2)),axis=1)\n",
        "\n",
        "Y1= np.sum(np.transpose(Y)\n",
        "           .reshape((8,2)),axis=1).reshape(4,2)\n",
        "print \"Answer: \",Y1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95L3lXh6nNNB",
        "colab_type": "text"
      },
      "source": [
        "### Exercise A.2.2\n",
        "\n",
        "Oftentimes we find that neutral networks work best when their input is mostly between 0 and 1. Below, we create a random dataset that is normal distributed (mean of 4, sigma of 10). Shift the data so that the mean is 0.5 and 68% of the data lies between 0 and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLyzLT9AnNNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=np.random.normal(4,10,1000)\n",
        "print np.mean(X)\n",
        "print np.min(X)\n",
        "print np.max(X)\n",
        "print np.var(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4JFKu3HCHhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVqaqawanNNE",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import math\n",
        "X1=(X-np.mean(X))/math.sqrt(np.var(X)) # Replace X with your answer\n",
        "\n",
        "print np.mean(X1)\n",
        "print np.var(X1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYXTCVDxnNNK",
        "colab_type": "text"
      },
      "source": [
        "<a id='MachineLearning'></a>\n",
        "# B. Machine Learning\n",
        "\n",
        "For the remainder of this tutorial, we will attempt to follow the first paper on Deep Learning in High Energy physics [P. Baldi, et al](https://arxiv.org/pdf/1402.4735.pdf). This paper demonstrates that Deep Neural Networks can learn from raw data the features that are typically used for searches for exotics particles. The authors publically provide the two benchmark scenarios considered in the paper. We will focus on the SUSY benchmark. \n",
        "\n",
        "<a id='Dataset'></a>\n",
        "## 1. The Dataset\n",
        "\n",
        "The data is distributed as a comma separated values (CSV) file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWsWNgUnNNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "2dc72d3c-e860-4df0-a3a1-4cf474d1550e"
      },
      "source": [
        "# Note filenames are defined in cell at top of this tutorial\n",
        "\n",
        "# print out the first 5 lines using unix head command (note in jupyter ! => shell command)\n",
        "SUSY_filename=\"SUSY-small.csv\"\n",
        "!head -5 $SUSY_filename"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000000000000000000e+00,9.728614687919616699e-01,6.538545489311218262e-01,1.176224589347839355e+00,1.157156467437744141e+00,-1.739873170852661133e+00,-8.743090629577636719e-01,5.677649974822998047e-01,-1.750000417232513428e-01,8.100607395172119141e-01,-2.525521218776702881e-01,1.921887040138244629e+00,8.896374106407165527e-01,4.107718467712402344e-01,1.145620822906494141e+00,1.932632088661193848e+00,9.944640994071960449e-01,1.367815494537353516e+00,4.071449860930442810e-02\n",
            "1.000000000000000000e+00,1.667973041534423828e+00,6.419061869382858276e-02,-1.225171446800231934e+00,5.061022043228149414e-01,-3.389389812946319580e-01,1.672542810440063477e+00,3.475464344024658203e+00,-1.219136357307434082e+00,1.295456290245056152e-02,3.775173664093017578e+00,1.045977115631103516e+00,5.680512785911560059e-01,4.819284379482269287e-01,0.000000000000000000e+00,4.484102725982666016e-01,2.053557634353637695e-01,1.321893453598022461e+00,3.775840103626251221e-01\n",
            "1.000000000000000000e+00,4.448399245738983154e-01,-1.342980116605758667e-01,-7.099716067314147949e-01,4.517189264297485352e-01,-1.613871216773986816e+00,-7.686609029769897461e-01,1.219918131828308105e+00,5.040258169174194336e-01,1.831247568130493164e+00,-4.313853085041046143e-01,5.262832045555114746e-01,9.415140151977539062e-01,1.587535023689270020e+00,2.024308204650878906e+00,6.034975647926330566e-01,1.562373995780944824e+00,1.135454416275024414e+00,1.809100061655044556e-01\n",
            "1.000000000000000000e+00,3.812560737133026123e-01,-9.761453866958618164e-01,6.931523084640502930e-01,4.489588439464569092e-01,8.917528986930847168e-01,-6.773284673690795898e-01,2.033060073852539062e+00,1.533040523529052734e+00,3.046259880065917969e+00,-1.005284786224365234e+00,5.693860650062561035e-01,1.015211343765258789e+00,1.582216739654541016e+00,1.551914215087890625e+00,7.612152099609375000e-01,1.715463757514953613e+00,1.492256760597229004e+00,9.071890264749526978e-02\n",
            "1.000000000000000000e+00,1.309996485710144043e+00,-6.900894641876220703e-01,-6.762592792510986328e-01,1.589282631874084473e+00,-6.933256387710571289e-01,6.229069828987121582e-01,1.087561845779418945e+00,-3.817416727542877197e-01,5.892043709754943848e-01,1.365478992462158203e+00,1.179295063018798828e+00,9.682182073593139648e-01,7.285631299018859863e-01,0.000000000000000000e+00,1.083157896995544434e+00,4.342924803495407104e-02,1.154853701591491699e+00,9.485860168933868408e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGGkeNunNNP",
        "colab_type": "text"
      },
      "source": [
        "Each row represents a LHC collision event. Each column contains some observable from that event. The variable names are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRZR6LqOnNNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VarNames=[\"signal\", \"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1rVvj1JnNNU",
        "colab_type": "text"
      },
      "source": [
        "Some of these variables represent the \"raw\" kinematics of the observed final state particles, while others are \"features\" that are derived from these raw quantities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouIdBpKFnNNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RawNames=[\"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\"]\n",
        "FeatureNames=[ \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73g2Lr2nNNX",
        "colab_type": "text"
      },
      "source": [
        "<a id='Pandas'></a>\n",
        "## 2. Pandas\n",
        "\n",
        "CSV is a common data format in Data Science, and there are lots of tools for loading them into memory in python. For a walk-through of some of these tools, please go through [Data Preparation Load Dataset notebook](https://github.com/afarbin/INFN-ML-Course/blob/master/DataPreparation__LoadDataset.ipynb).\n",
        "\n",
        "We will use [pandas](http://pandas.pydata.org) to read in the file, and [matplotlib](https://matplotlib.org) to make plots. Pandas provides \"data structures and data analysis tools for the Python Programming Language\". Many machine learning tasks can be accomplished with [numpy](http://www.numpy.org) tensors and [h5py](http://www.h5py.org) files. In this case, pandas just makes it very easy to read a CSV file.\n",
        "\n",
        "The following ensures pandas is installed and sets everything up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR4luur-nNNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb7SQudNnNNa",
        "colab_type": "text"
      },
      "source": [
        "Now we can read the data into a pandas dataframe. It's a ~GB file, so be patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBghUbCcnNNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(SUSY_filename, dtype='float64', names=VarNames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vaKEl89nNNc",
        "colab_type": "text"
      },
      "source": [
        "Another nice feature of pandas is that you can see the data in Jupyter by just evaluating the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "72sprSXvnNNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_95bNlPnNNe",
        "colab_type": "text"
      },
      "source": [
        "The first column stores the \"truth\" label of whether an event was signal or background. Pandas makes it easy to create dataframes that store only the signal or background events:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDpOOdJ7nNNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sig=df[df.signal==1]\n",
        "df_bkg=df[df.signal==0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEqL0BKXnNNh",
        "colab_type": "text"
      },
      "source": [
        "The following example plots the signal and background distributions of every variable. Note that we use VarNames[1:] to skip the first variable, which was the true label. \n",
        "\n",
        "We will use matplotlib for plotting. There are lots of tutorials and primers out there that you can find searching the web. A good tutorial can be found in the [Scipy Lectures](http://www.scipy-lectures.org/intro/matplotlib/matplotlib.html). Look through these on your own time, it is not necessary for doing these exercise. The code below is all you need to know for making histograms with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mCeEs9KnNNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for var in VarNames[1:]:\n",
        "    print var\n",
        "    plt.figure()\n",
        "    plt.hist(np.array(df_sig[var]),bins=100,histtype=\"step\", color=\"red\",label=\"background\",stacked=True)\n",
        "    plt.hist(np.array(df_bkg[var]),bins=100,histtype=\"step\", color=\"blue\", label=\"signal\",stacked=True)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHx4d_dAnNNk",
        "colab_type": "text"
      },
      "source": [
        "<a id='Scikit-learn'></a>\n",
        "## 3. Scikit-learn\n",
        "\n",
        "[Scikit-learn](http://scikit-learn.org) is a rich python library for data science, including machine learning. As an example, we can easily build a Fisher Discriminant (aka Linear Discriminant Analysis, or LDA). The [LDA Documentation](http://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis) does as great job explaining this classifier. Here's how we instanciate the classifier: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHqbj_snNNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.discriminant_analysis as DA\n",
        "Fisher=DA.LinearDiscriminantAnalysis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHb4qd5FnNNm",
        "colab_type": "text"
      },
      "source": [
        "Lets separate the data into inputs (X) vs outputs (Y) and training vs testing samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kVXZ7KZnNNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_Train=4000000\n",
        "\n",
        "Train_Sample=df[:N_Train]\n",
        "Test_Sample=df[N_Train:]\n",
        "\n",
        "X_Train=Train_Sample[VarNames[1:]]\n",
        "y_Train=Train_Sample[\"signal\"]\n",
        "\n",
        "X_Test=Test_Sample[VarNames[1:]]\n",
        "y_Test=Test_Sample[\"signal\"]\n",
        "\n",
        "Test_sig=Test_Sample[Test_Sample.signal==1]\n",
        "Test_bkg=Test_Sample[Test_Sample.signal==0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFOKx0dHnNNp",
        "colab_type": "text"
      },
      "source": [
        "We can train the classifier as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mozDMu8WnNNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fisher.fit(X_Train,y_Train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QIcHu5EnNNq",
        "colab_type": "text"
      },
      "source": [
        "We can plot the output, comparing signal and background:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoUC7rAqnNNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.hist(Fisher.decision_function(Test_sig[VarNames[1:]]),bins=100,histtype=\"step\", color=\"blue\", label=\"signal\",stacked=True)\n",
        "plt.hist(Fisher.decision_function(Test_bkg[VarNames[1:]]),bins=100,histtype=\"step\", color=\"red\", label=\"background\",stacked=True)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMpGmqFgnNNr",
        "colab_type": "text"
      },
      "source": [
        "And we can make a ROC curve and evaluate the AUC:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3slujk7nNNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, _ = roc_curve(y_Test, Fisher.decision_function(X_Test))\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaGknupanNNt",
        "colab_type": "text"
      },
      "source": [
        "## Exercise B.3.1\n",
        "\n",
        "Train the Fisher performance using the raw, features, and raw+features as input. Compare the performance one a single plot. Add cells to this notebook as needed. Or start new notebooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjJ9n4MnNNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Train_Raw=Train_Sample[RawNames]\n",
        "X_Test_Raw=Test_Sample[RawNames]\n",
        "\n",
        "X_Train_Features=Train_Sample[FeatureNames]\n",
        "X_Test_Features=Test_Sample[FeatureNames]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nvj0-OdnNNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TrainFisher(X_Train,X_Test,y_Train):\n",
        "    Fisher=DA.LinearDiscriminantAnalysis()\n",
        "    Fisher.fit(X_Train,y_Train)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_Test, Fisher.decision_function(X_Test))\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.show()\n",
        "    \n",
        "    return Fisher\n",
        "\n",
        "RawFisher=TrainFisher(X_Train_Raw,X_Test_Raw,y_Train)\n",
        "FeatureFisher=TrainFisher(X_Train_Features,X_Test_Features,y_Train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMzUKZuwnNNw",
        "colab_type": "text"
      },
      "source": [
        "## Exercise B.3.2\n",
        "\n",
        "Select 3 different classifiers from the techniques listed [here](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Note that you can use the multi-layer peceptron to build a deep network, though training may be prohibitively slow, so avoid this technique. Perform the comparison in exercise 1 for each classifier. Compare your conclusions for your selected techniques to the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Ly0DGCnNNx",
        "colab_type": "text"
      },
      "source": [
        "## Exercise B.3.3\n",
        "\n",
        "The following function calculates the significance of the observation of the signal given the number of expected Signal and Background events, using the simple formula $\\sigma_S= \\frac{N_S}{\\sqrt{N_S+N_B}}$. Read through the code carefully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFEkexIznNNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PlotSignificance(N_S,N_B, N_S_min=1):\n",
        "    plt.figure()\n",
        "    eff_sig,bins_sig,p_sig=plt.hist(Fisher.decision_function(Test_sig[VarNames[1:]]),bins=100,histtype=\"step\", color=\"blue\", label=\"signal\",cumulative=-1,stacked=True,normed=True)\n",
        "    eff_bkg,bins_bkg,p_bkg=plt.hist(Fisher.decision_function(Test_bkg[VarNames[1:]]),bins=100,histtype=\"step\", color=\"red\", label=\"background\",cumulative=-1,stacked=True,normed=True)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "    \n",
        "    good_bins = np.where(eff_sig*N_S>=N_S_min)\n",
        "\n",
        "    print len(good_bins[0])\n",
        "    if len(good_bins[0])<1:\n",
        "        print \"Insufficient Signal.\"\n",
        "        return 0,0,0\n",
        "    \n",
        "    significance=(N_S*eff_sig)/np.sqrt((N_B*eff_bkg)+(N_S*eff_sig))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(bins_sig[:-1],significance)\n",
        "    \n",
        "    max_sign=np.max(significance[good_bins])\n",
        "    max_signI=np.argmax(significance[good_bins])\n",
        "    \n",
        "    plt.show()\n",
        "    print \"Max significance at \", bins_sig[max_signI], \" of\", max_sign\n",
        "    return bins_sig[max_signI],max_sign, max_signI\n",
        "    \n",
        "PlotSignificance(1000000,1e11)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Hg2MAZnNNz",
        "colab_type": "text"
      },
      "source": [
        "Answer the following questions:\n",
        "   * What are we computing when making a normalized cummulative plot? \n",
        "   * Assume that the experiment produces 1 signal event for every $10^{11}$ background events. For each of your classifiers, how many signal events need to be produced to be able to make a $5\\sigma$ discovery claim?\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsCtDkInNNJ",
        "colab_type": "text"
      },
      "source": [
        "<a id='h5py'></a>\n",
        "## 4. h5py\n",
        "\n",
        "[HDF5](https://support.hdfgroup.org/HDF5/) is a \"data model, library, and file format for storing and managing data.\" It is also the most common storage format in data science. [h5py](http://www.h5py.org) provides a python API for HDF5. In most cases, you do not need to know very much about HDF5 or h5py, just how to read/write tensors into/from files, which you can easily pick up from the [h5py Quick Start](http://docs.h5py.org/en/latest/quick.html#quick). We won't be using HDF5 for this tutorial. This section is here as reference for when you do encounter an HDF5 file.\n",
        "\n",
        "Here's an example of how you can save the data we are using into an h5 file:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRnmz_NlLefK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "hf = h5py.File('SUSY.h5', 'w')\n",
        "hf.create_dataset('Events', data=df)\n",
        "hf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8B7wTg0RugF",
        "colab_type": "text"
      },
      "source": [
        "And here's how you \"read\" it back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXdEXHo4R16I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hf = h5py.File('SUSY.h5')\n",
        "print hf.keys()\n",
        "\n",
        "my_data=hf[\"Events\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE2R9BcFSFlM",
        "colab_type": "text"
      },
      "source": [
        "Note while the data you are reading back is the same, it was stored in a pandas dataframe before and now is in h5 data object. One important difference is that the h5 doesn't actually load the data into memory until you access it, which is can be useful for very large datasets. You can convert the data into a numpy tensor or a pandas dataframe, but then the data will be loaded into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFIWjttfnNNz",
        "colab_type": "text"
      },
      "source": [
        "<a id='DeepLearning'></a>\n",
        "# C. Deep Learning\n",
        "\n",
        "This section is meant to get you started in using Keras to design Deep Neural Networks. The goal here is to simply repeat section B with Deep Learning.\n",
        "\n",
        "If you are starting here and have not run the cells above that load the data, you will need to run the following cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZVEVofvnNNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "VarNames=[\"signal\", \"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]\n",
        "RawNames=[\"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\"]\n",
        "FeatureNames=[ \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]\n",
        "\n",
        "df = pd.read_csv(SUSY_filename, dtype='float64', names=VarNames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQXMeLQ1nNN0",
        "colab_type": "text"
      },
      "source": [
        "Now lets define training and test samples. Note that DNNs take very long to train, so for testing purposes we will use only about 10% of the 5 million events in the training/validation sample. Once you get everything working, you can go back and make the final version of your plots with the full sample. \n",
        "\n",
        "Also note that Keras had trouble with the Pandas tensors, so after doing all of the nice manipulation that Pandas enables, we convert the Tensor to a regular numpy tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqHFHnGnNN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_Max=550000\n",
        "N_Train=500000\n",
        "\n",
        "Train_Sample=df[:N_Train]\n",
        "Test_Sample=df[N_Train:N_Max]\n",
        "\n",
        "X_Train=np.array(Train_Sample[VarNames[1:]])\n",
        "y_Train=np.array(Train_Sample[\"signal\"])\n",
        "\n",
        "X_Test=np.array(Test_Sample[VarNames[1:]])\n",
        "y_Test=np.array(Test_Sample[\"signal\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_NlPgg-nNN2",
        "colab_type": "text"
      },
      "source": [
        "<a id='Keras'></a>\n",
        "## 1. Keras\n",
        "\n",
        "Training Deep Learning models can take a very long time. If you have access to a GPU, training with the GPU will be about 2 orders of magnitude faster that training with just the CPU. Unforunately, there are no GPUs on lxplus. But, if you are running this notebook on a system with NVidia GPU(s) properly setup, you can tell Keras to use a specific GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0rNZUXgnNN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since lxplus does not have any GPUs, please DO NOT RUN THIS CELL ON LXPLUS.\n",
        "# Selecting First GPU in the system\n",
        "\n",
        "import os\n",
        "\n",
        "# Selecting GPU manually:\n",
        "gpuid= 0\n",
        "\n",
        "# If running on UTA DL Cluster and using GPU:\n",
        "# print \"Using Queue:\", os.environ[\"PBS_QUEUE\"]\n",
        "# gpuid=int(os.environ[\"PBS_QUEUE\"][3:4])\n",
        "\n",
        "print \"Selected GPU:\", gpuid\n",
        "\n",
        "# If Using TensorFlow:\n",
        "import tensorflow as tf\n",
        "# Comment if using GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "config = tf.ConfigProto(intra_op_parallelism_threads=0,\n",
        "                        inter_op_parallelism_threads=0,\n",
        "                        allow_soft_placement=True,\n",
        "                        # Comment if using GPU\n",
        "                        device_count={'CPU' : 1, 'GPU' : 0},                                                \n",
        "                        # Uncomment to use GPU\n",
        "                        # gpu_options=tf.GPUOptions(visible_device_list=\\\"{}\\\".format(gpu_id),\n",
        "                        #                          force_gpu_compatible=True,\n",
        "                        #                          allow_growth=True)\n",
        "                       )\n",
        "\n",
        "# If using Theano and GPU:\n",
        "# os.environ['THEANO_FLAGS'] = \"\" # \"mode=FAST_RUN,device=gpu%s,floatX=float32,force_device=True\" % (gpuid)\n",
        "# import theano\n",
        "# theano.config.profile=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3qZ0U4AnNN4",
        "colab_type": "text"
      },
      "source": [
        "Now we will build a simple model. Note that this is a very small model, so things run fast. You should attempt more ambitious models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgtmA4CcnNN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_Train.shape[1], init='uniform', activation='relu'))\n",
        "model.add(Dense(8, init='uniform', activation='relu'))\n",
        "model.add(Dense(1, init='uniform', activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5InqaOj9nNN6",
        "colab_type": "text"
      },
      "source": [
        "The model has to be compiled. At this time we set the loss function and the optimizer too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNm3rChnNN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOi2w-gWnNN8",
        "colab_type": "text"
      },
      "source": [
        "Now we train. We are running only 10 epochs in this example. Models may need hundreds of epochs before they stop improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh_zqOwWnNN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit(X_Train, y_Train, validation_data=(X_Test,y_Test), nb_epoch=10, batch_size=2048)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzdUQKknNN-",
        "colab_type": "text"
      },
      "source": [
        "The model history keeps track of the loss and accuracy for each epoch. Note that the training above was setup to run on the validation sample at the end of each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBtzkyE1nNN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqlD1aT7nNOA",
        "colab_type": "text"
      },
      "source": [
        "You can plot the loss versus epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN0lE7kbnNOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_history=history.history[\"loss\"]\n",
        "plt.plot(range(len(loss_history)),loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZDE6xB1nNOB",
        "colab_type": "text"
      },
      "source": [
        "## Exercise C.1.1\n",
        "\n",
        "You will need to create several models and make sure they are properly trained. Write a function that takes this history and plots the values versus epoch. For every model that you train in the remainder of this lab, assess:\n",
        "\n",
        "    * Has you model's performance plateaued? If not train for more epochs. \n",
        "\n",
        "    * Compare the performance on training versus test sample. Are you over training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZFn80pLnNOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your Solution Here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVVy9VpinNOC",
        "colab_type": "text"
      },
      "source": [
        "## Exercise C.1.2\n",
        "\n",
        "Explicitly overtrain your model by running for a large number of epochs. Demonstrate that you are over-fitting. Then add dropout layers to your model to alleviate the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfVEG432nNOC",
        "colab_type": "text"
      },
      "source": [
        "## Exercise C.1.3\n",
        "\n",
        "Following section B, make a comparison of the performance between models trained with raw, features, and raw+features data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVDUUBj4nNOD",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate how the trained model does on the test sample as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnN7EOvVnNOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_Test, y_Test)\n",
        "print scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZLpdC-nNOE",
        "colab_type": "text"
      },
      "source": [
        "And we can make ROC curves as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-qUjGv3nNOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, _ = roc_curve(y_Test, model.predict(X_Test))\n",
        "                        \n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JMK5yH3nNOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your solution here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azgmNCJenNOG",
        "colab_type": "text"
      },
      "source": [
        "## Exercise C.1.4\n",
        "\n",
        "Again, following section B, design and implement at least 3 different DNN models. Train them and compare performance. You may try different architectures, loss functions, and optimizers to see if there is an effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0GNsbg0nNOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9aHIzownNOI",
        "colab_type": "text"
      },
      "source": [
        "## Exercise C.1.5\n",
        "\n",
        "Write a function that evaluates the performance (AUC) as a function of a given input variable. You will need to bin the test data in the variable (i.e. make sub-samples for events which have the particular variable in a range), evaluate the performance in each bin, and plot the results.\n",
        "\n",
        "Apply your function to each input variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOwIuZvbnNOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RxLT618nNOK",
        "colab_type": "text"
      },
      "source": [
        "# HiggsML Dataset\n",
        "\n",
        "The HiggsML challenge was organized by ATLAS colleagues and ran on Kaggle's platform. The challenge provided a training and test set of Higgs and background events in CSV format. Using this data, participants were tasked with creating a classifier, which they submitted to Kaggle. Kaggle evaluated the classifier against another test set. At the end of the competition, the best performing classifiers were awarded a cash prize.\n",
        "\n",
        "The challenge is described in https://higgsml.lal.in2p3.fr\n",
        "\n",
        "The Kaggle site is https://www.kaggle.com/c/higgs-boson\n",
        "\n",
        "Detail description of the data and challenge: https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf\n",
        "\n",
        "You are welcome to use Scikit or any other package you like.\n",
        "\n",
        "Please separate different steps into different Jupyter Notebooks. For example:\n",
        "\n",
        "A copy of the data CSV files are on the cluster at: /data/afarbin/DLClass/HiggsML\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O8xg4CsnNOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}